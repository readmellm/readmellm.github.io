<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ReadMe LLM</title>
    <link rel="stylesheet" href="assets/css/normalize.css">
    <link rel="stylesheet" href="assets/css/style.css">
    <link rel="stylesheet" href="assets/css/workflow.css">
    <link rel="icon" type="image/x-icon" href="assets/images/favicon.ico">
</head>

<body>
    <header class="hero">
        <h1>ReadMe LLM</h1>
        <p>A Framework to Contextualize Software Libraries for LLMs</p>
    </header>

    <nav class="navbar">
        <div class="nav-container">
            <!-- <a href="index.html" class="nav-logo">ReadMe LLM</a> -->
            <ul class="nav-menu">
                <li><a href="#solution">ReadMe.LLM</a></li>
                <li><a
                        href="https://docs.google.com/document/d/14gC1LOyP63WjCu_2aQLGns0_pIveCl24LfAfc8tymPA/edit?tab=t.0#heading=h.81n4a8x05dl1">Pre-print</a>
                </li>
                <li><a href="#demo-video">Demo Video</a></li>
                <li><a href="#workflow">Try it Out!</a></li>
                <li><a href="#social-showcase">Showcase</a></li>
                <li><a href="#team">Team</a></li>
                <!--                 <li><a href="#library">Library Maker</a></li>
                <li><a href="#developer">Developer</a></li> -->
                <li><a href="#contact">Contact</a></li>
                <!-- <li><a href="">Discussion</a></li> -->
            </ul>
        </div>
    </nav>

    <main>
        <section id="problem">
            <h2>TL;DR</h2>
            <div class="tldr">
                <div class="problem-statement">
                    <p>
                        <!-- <b>Problem:</b> LLMs often struggle with newer or lesser-known libraries due to limited online 
                    documentation or lack of respresentation in their training data.  -->
                        <b>Problem: </b> Coding LLMs often struggle with newer or lesser-known libraries due to limited
                        online documentation and/or lack of representation in their training data.

                    </p>
                    <p>
                        <b>Solution:</b> We need LLM oriented documentation. ReadMe.LLM is a solution that addresses
                        this issue by providing structured descritpioins
                        of the codebase and other metadata. Library developers provide ReadMe.LLM and engineers
                        use them by simply copy pasting its contents into their chat window along with their query.
                    </p>
                </div>
                <div class="compare-graph">
                    <img src="assets/images/compare.png" alt="ReadMe LLM framework">
                </div>
            </div>
        </section>

        <section>

            <h2>Bridging the LLM Knowledge Gap</h2>
            <!-- <img src="assets/images/problem.png" alt="ReadMe LLM framework" class="center"> -->
            <h3>The Challenge</h3>
            <!-- <img src="assets/images/problem.png" alt="ReadMe LLM framework" class="center"> -->
            <!-- <p>Many developers use LLMs to generate code with existing software libraries, leveraging these tools to
                    speed up development or simplify onboarding.
                    However, not all libraries are equally represented in LLM training data. Newer or smaller libraries
                    often lack online documentation, making it difficult for LLMs to generate accurate code leading to
                    often misused or misrepresented in AI-generated code.
                    In contrast, well-established libraries like Pandas have extensive resources that help models
                    produce reliable output.</p> -->
            <p>Developers use LLMs to generate code, and for many reasons, good code uses libraries. Not every library
                is like Pandas which has a lot of documentation, questions on Stack Overflow, and other resources.
                Newer or smaller libraries that target a niche domain often lack online documentation, making it
                difficult for LLMs to generate accurate code.</p>
            <!-- New Case Studies Section -->
            <div class="case-studies">
                <h3>Real-World Examples: When LLMs Fail</h3>

                <p>This is not a theoretical concern. We initially conducted experiments without any context across
                    multiple LLMs:</p>

                <table class="llm-table">
                    <thead>
                        <tr>
                            <th>LLM</th>
                            <th>Training Cutoff Date</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><a href="https://openai.com/index/hello-gpt-4o/" target="_blank"
                                    rel="noopener noreferrer">GPT-4o</a></td>
                            <td>October 2023</td>
                        </tr>
                        <tr>
                            <td><a href="https://sonar.perplexity.ai" target="_blank" rel="noopener noreferrer">Sonar
                                    (Llama3 70B)</a></td>
                            <td>December 2023</td>
                        </tr>
                        <tr>
                            <td><a href="https://www.anthropic.com/claude/sonnet" target="_blank"
                                    rel="noopener noreferrer">Claude 3.7 Sonnet</a></td>
                            <td>October 2024</td>
                        </tr>
                        <tr>
                            <td><a href="https://x.ai/news/grok-2" target="_blank" rel="noopener noreferrer">Grok-2</a>
                            </td>
                            <td>July 2024</td>
                        </tr>
                        <tr>
                            <td><a href="https://api-docs.deepseek.com/news/news250120" target="_blank"
                                    rel="noopener noreferrer">DeepSeek R1</a></td>
                            <td>July 2024</td>
                        </tr>
                    </tbody>
                </table>

                We test two distinct libraries:
                <ul>
                    <li>
                        <a href="https://supervision.roboflow.com/latest/" target="_blank" rel="noopener noreferrer">
                            Supervision
                        </a>: A modern, industry-run library created in 2022 with 26.4k stars on Github. This library is
                        a computer vision library that is widely used, but is also very recently developed, possibly
                        causing it to be not included in a LLM's training data.
                    </li>
                    <li>
                        <a href="https://github.com/MITHaystack/digital_rf" target="_blank" rel="noopener noreferrer">
                            DigitalRF
                        </a>: An academic software library created in 2017 with 106 stars on GitHub. This library has
                        limited documentation and tests LLMs ability to handle libraries they are unlikely to have seen
                        during training.
                    </li>

                </ul>
                </p>

                <!-- Case Study 1 -->
                <div class="case-study">
                    <h4>Case Study 1: Supervision</h4>
                    <div class="case-details">
                        <div class="case-description">
                            <p><strong>Task:</strong> For Supervision, we tasked LLMs with detecting and annotating cars
                                in an image. We selected an image with multiple objects (such as people, or buildings)
                                to introduce complexity, and the LLM-generated code had to identify all of the cars in
                                the image, add a confidence score annotation, save the bounding box coordinates, and
                                crop each detected car. </p>
                        </div>
                        <div class="case-outcome">
                            <p><strong>Key Results:</strong> When attempting this task with no additional context, we
                                observed that most of the models had mimal success rates ranging from just 0% to 20%.
                                The sole exception was DeepSeek R1 which had an impressive success rate of 80%, 
                                since it has clearly been trained on the Supervision library. </p>
                            <img src="assets/images/Supervision_NoContext.png" alt="ReadMe LLM framework"
                                class="center">

                        </div>
                    </div>
                </div>

                <!-- Case Study 2 -->
                <div class="case-study">
                    <h4>Case Study 2: DigitalRF</h4>
                    <div class="case-details">
                        <div class="case-description">
                            <p><strong>Task:</strong> An example of a common task is a format translation task. For
                                DigitalRF, we tasked the LLMs with writing a WAV file into a DigitalRF specified HDF5
                                format. We obtained a WAV file (a 10-second long radio signal) containing I/Q data using
                                the SDR++ application and tasked LLMs with converting it to a standardized HDF5 format.
                            </p>
                        </div>
                        <div class="case-outcome">
                            <p><strong>Key Results:</strong>Without adding additional context through ReadMe.LLM, we
                                observed poor performances across all 5 of the models ranging from just 0% to 40%
                                success rates.</p>
                            <img src="assets/images/DigitalRF_NoContext.png" alt="ReadMe LLM framework" class="center">
                        </div>
                    </div>
                </div>


            </div>
            <!-- End of Case Studies Section -->

            <div class="impact">
                <h3>So LLM's don't give proper generated code, Who's Impacted?</h3>
                <div class="two-column">
                    <div class="column">
                        <h4>Engineers</h4>
                        <ul>
                            <li>Receive incorrect or non-functional code when starting a project or something new</li>
                            <li>Experience frustration and prolonged debugging</li>
                            <li>Increase company resource expenditure</li>
                        </ul>
                    </div>
                    <div class="column">
                        <h4>Library Developers</h4>
                        <ul>
                            <li>Risk losing potential users</li>
                            <li>See developers abandon their tools</li>
                            <li>Compete against alternatives that work seamlessly with LLMs</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="ecosystem-impact">
                <h3>Ecosystem Consequences</h3>
                <!-- <p>This dynamic is reshaping the software ecosystem—smaller libraries risk lack-of-use not due to their
                    technical merit but because LLMs fail to represent them accurately. For developers, this means fewer
                    viable options and slower innovation.</p> -->
                <p>
                    LLMs are reshaping the software ecosystem. 
                    What's not appreciated is that this changes the ecosystem for libraries in particular. 
                    Smaller/niche libraries lose potential users not due to their technical merit 
                    but because LLMs fail to represent their capabilities accurately. 
                    For developers, this means fewer viable options and slower innovation. 
                    For AI agents, this means poorer performance. 
                    Our solution is a framework that ensures LLMs can correctly understand and utilize any software library, 
                    leveling the playing field and fostering a more diverse, innovative, and accessible development landscape.
                </p>

                <!-- <p>Many developers use LLMs to generate code with existing software libraries, leveraging these tools to speed up development or simplify onboarding. However, not all libraries are equally represented in LLM training data.  Newer or smaller libraries often lack online documentation, making it difficult for LLMs to generate accurate code. In contrast, well-established libraries like Pandas have extensive resources that help models produce reliable output, while lesser-known libraries are often misused or misrepresented in AI-generated code.
                This gap negatively impacts both engineers and library developers. Engineers receive incorrect or non-functional code, leading to frustration, prolonged debugging, and increased company resource expenditure. Meanwhile, library developers risk losing potential users as developers abandon their tools in favor of alternatives that work seamlessly with LLM-generated code. 
                This dynamic is reshaping the software ecosystem—smaller libraries risk obsolescence not due to their technical merit but because LLMs fail to represent them accurately. For developers, this means fewer viable options and slower innovation. Our solution is a framework that ensures LLMs can correctly understand and utilize any software library, leveling the playing field and fostering a more diverse, innovative, and accessible development landscape.
                </p> -->
        </section>

        <section id="solution">
            <h2>ReadMe.LLM</h2>
            <!-- <img src="assets/images/solution.png" alt="ReadMe LLM framework" class="center"> -->

            <p>We propose ReadMe.LLM, a structured framework that helps library makers make their tools more accessible
                to LLMs:</p>
            <ul>
                <li>Optimized Documentation for LLMs: ReadMe.LLM provides structured descriptions of the codebase and
                    other metadata.</li>
                <li>Seamless Integration: Library developers attach ReadMe.LLM to their codebase, allowing LLMs to
                    accurately utilize the library without requiring additional fine-tuning.</li>
                <li>Enhanced Developer Experience: Developers simply copy and paste the ReadMe.LLM contents into the LLM
                    chat window and ask their query. The LLM now having better context about the library—can select
                    appropriate functions to correctly implement users' design.</li>
            </ul>
            <p>Our approach shifts the focus from fixing LLMs' limitations to empowering libraries to be LLM-friendly,
                fostering adoption of emerging libraries. The respective workflows for a Library Developer and Engineer
                are illustrated below:</p>
            <p><b>Engineer workflow:</b></p>
            <img src="assets/images/workflow2.png" alt="User Chart" class="center">
            <p><b>Library Developer workflow:</b></p>
            <img src="assets/images/workflow1.png" alt="User Chart" class="center">


            <div class="case-studies">
                <h3>ReadMe.LLM's Success</h3>
                <p>We repeated the experiments on DigitalRF and Supervision mentioned in the previous section, and we
                    observed significantly better results using the ReadMe.LLM framework:</p>
                <!-- Case Study 1 -->
                <div class="case-study">
                    <h4>Case Study 1: Supervision</h4>
                    <div class="case-details">
                        <div class="case-description">
                            <!-- <p><strong>Task:</strong> For Supervision, we tasked LLMs with detecting and annotating cars in an image. We selected an image with multiple objects (such as people, or buildings) to introduce complexity and test the LLMs' ability to differentiate between relevant and irrelevant detections. The LLM had to identify all cars, add a confidence score annotation, save the bounding box coordinates, and crop each detected car. </p> -->
                        </div>
                        <div class="case-outcome">
                            <p><strong>Key Results:</strong> We created an optimal ReadME.LLM for Supervision, and as
                                can be seen in the graph below, we were able to reach 100% success rate across all 5
                                models for this task.</p>
                            <img src="assets/images/Success_Supervision.png" alt="ReadMe LLM framework" class="center">
                        </div>
                    </div>
                </div>
                <!-- Case Study 2 -->
                <div class="case-study">
                    <h4>Case Study 2: DigitalRF</h4>
                    <div class="case-details">
                        <div class="case-description">
                            <!-- <p><strong>Task:</strong> For DigitalRF, we tasked the LLMs with writing a WAV file into HDF5 format. We obtained a WAV file (a 10-second long radio signal) containing I/Q data using the SDR++ application and tasked LLMs with converting it to a standardized HDF5 format. </p> -->
                        </div>
                        <div class="case-outcome">
                            <p><strong>Key Results:</strong> After experimenting with several different types of
                                ReadMe.LLM's we observed that each model had a ReadMe.LLM that resulted in a 100%
                                success rate.</p>
                            <img src="assets/images/Success_DigitalRF.png" alt="ReadMe LLM framework" class="center">
                        </div>
                    </div>
                </div>
            </div>
            <!-- End of Case Studies Section -->
        </section>

        <section id="demo-video">
            <h2>Demo Video</h2>
            <p>Watch our demonstration of how ReadMe.LLM helps improve code generation for engineers. You can access all
                of the materials used in the video in the below "Try it Out!" section.</p>
            <div class="video-container">
                <video width="800" height="450" controls poster="assets/images/thumbnail.png">
                    <source src="assets/videos/FinalVideo.mp4" type="video/mp4">
                    <source src="assets/videos/FinalVideo.webm" type="video/webm">
                    Your browser does not support the video tag.
                </video>
            </div>
        </section>


        <section id="workflow">
            <h2>Try it Out!</h2>
            <p>Follow the steps in the screen shot below to generate high quality code with the help of ReadMe.LLM! </p>
            <img src="assets/images/github_to_gpt.png" alt="User Chart" class="center large-image">
            <img src="assets/images/gpt_to_code.png" alt="User Chart" class="center large-image">

            <p>Here is an example task using the Supervision Library, feel free to try it <b>with and without
                    ReadMe.LLM</b>! See what happens</p>
            <p><b>Task:</b></p>
            <div class="code-container">
                <button class="copy-button" onclick="copyCode()">Copy</button>
                <div id="codeBlock">
                    <p>Using the Supervision Library, find all the people in the “original_image”
                        and annotate them with a blur. Overlay the detected people in the original
                        image with “oski_image”. Print both the annotated image and the overlayed image.
                    </p>
                    <!-- &lt;div&gt;Hello, World!&lt;/div&gt;
              &lt;p&gt;This is a sample code block.&lt;/p&gt; -->
                </div>
            </div>
            <p><b>ReadMe.LLM:</b></p>
            <div class="code-container">
                <button class="copy-button" onclick="copyCode()">Copy</button>
                <div id="readme-content"></div>
            </div>
            <a href="assets/images/original_image.jpeg" download="original_image.jpeg" class="download-btn">
                <button>Download Original Image</button>
            </a>
            <a href="assets/images/oski_image.png" download="oski_image.png" class="download-btn">
                <button>Download Annotated Image</button>
            </a>
            <p>Here is the expected output is the code is correct</p>
            <img src="assets/images/blur.jpeg" alt="User Chart" class="center">
            <img src="assets/images/oski_people.jpeg" alt="User Chart" class="center">

        </section>

        <section id="social-showcase">
            <h2>Showcase</h2>
            <div class="social-container">
                <div class="twitter-feed">
                    <h3>Discussion on X</h3>
                    <div class="twitter-posts">
                        <blockquote class="twitter-tweet">
                            <p lang="en" dir="ltr">It&#39;s 2025 and most content is still written for humans instead of
                                LLMs. 99.9% of attention is about to be LLM attention, not human attention.<br><br>E.g.
                                99% of libraries still have docs that basically render to some pretty .html static pages
                                assuming a human will click through them.…</p>&mdash; Andrej Karpathy (@karpathy) <a
                                href="https://twitter.com/karpathy/status/1899876370492383450?ref_src=twsrc%5Etfw">March
                                12, 2025</a>
                        </blockquote>
                        <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
                        <blockquote class="twitter-tweet">
                            <p lang="en" dir="ltr">Good tip from Replit’s <a
                                    href="https://twitter.com/mattppal?ref_src=twsrc%5Etfw">@mattppal</a> at AI Dev 25
                                on debugging while vibe coding: Large part of it is looking at outputs to figure out
                                what context you have that LLM does not, so that you can give it that context and help
                                it get unstuck. Sometimes pasting in the error messages is… <a
                                    href="https://t.co/4HSLeoviLp">pic.twitter.com/4HSLeoviLp</a></p>&mdash; Andrew Ng
                            (@AndrewYNg) <a
                                href="https://twitter.com/AndrewYNg/status/1900617330906067136?ref_src=twsrc%5Etfw">March
                                14, 2025</a>
                        </blockquote>
                        <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
                    </div>
                </div>
                <!-- <div class="linkedin-feed">
                    <h3>LinkedIn Updates</h3>
                    <div class="linkedin-posts">
                        <div class="linkedin-badge">
                            <a href="https://www.linkedin.com/feed/update/urn:li:ugcPost:7289056858122682368" target="_blank">View LinkedIn Post</a>
                        </div>
                    </div>
                </div> -->
            </div>
        </section>


        <!-- Team Section -->
        <section id="team">
            <h2>Meet Our Team</h2>

            <div class="team-container">
                <!-- Team Member 1 -->
                <div class="team-member">
                    <img src="assets/images/gomez.jpg" alt="Team Member 1" class="team-member-image">
                    <div class="member-info">
                        <h3>Alejandro Gómez Soteres</h3>
                        <!-- <p class="member-bio">Short bio describing this team member's background, expertise, and contributions to the ReadMe LLM project.</p> -->
                        <div class="social-links">
                            <a href="https://www.linkedin.com/in/gs-alejandro/">
                                <img src="assets/images/linkedin-logo.png" alt="LinkedIn" class="social-icon">
                            </a>

                            <!-- <a href="#">GitHub</a> -->
                        </div>
                    </div>
                </div>

                <!-- Team Member 2 -->
                <div class="team-member">
                    <img src="assets/images/sandya1.jpg" alt="Team Member 2" class="team-member-image">
                    <div class="member-info">
                        <h3>Sandya Wijaya</h3>
                        <!-- <p class="member-bio">Short bio describing this team member's background, expertise, and contributions to the ReadMe LLM project.</p> -->
                        <div class="social-links">
                            <a href="https://www.linkedin.com/in/sandyawijaya/">
                                <img src="assets/images/linkedin-logo.png" alt="LinkedIn" class="social-icon">
                            </a>
                            <!-- <a href="#">GitHub</a> -->
                        </div>
                    </div>
                </div>

                <!-- Team Member 3 -->
                <div class="team-member">
                    <img src="assets/images/shri.JPG" alt="Team Member 3" class="team-member-image">
                    <div class="member-info">
                        <h3>Shriyanshu Kode</h3>
                        <!-- <p class="member-bio">Short bio describing this team member's background, expertise, and contributions to the ReadMe LLM project.</p> -->
                        <div class="social-links">
                            <a href="https://www.linkedin.com/in/shriyanshuk/">
                                <img src="assets/images/linkedin-logo.png" alt="LinkedIn" class="social-icon">
                            </a>
                            <!-- <a href="#">GitHub</a> -->
                        </div>
                    </div>
                </div>

                <!-- Team Member 4 -->
                <div class="team-member">
                    <img src="assets/images/jacob_headshot.jpeg" alt="Team Member 4" class="team-member-image">
                    <div class="member-info">
                        <h3>Jacob Bolano</h3>
                        <!-- <p class="member-bio">Short bio describing this team member's background, expertise, and contributions to the ReadMe LLM project.</p> -->
                        <div class="social-links">
                            <a href="https://www.linkedin.com/in/jacobbolano/">
                                <img src="assets/images/linkedin-logo.png" alt="LinkedIn" class="social-icon">
                            </a>
                            <!-- <a href="#">GitHub</a> -->
                        </div>
                    </div>
                </div>

                <!-- Team Member 5 -->
                <div class="team-member">
                    <img src="assets/images/yue.jpeg" alt="Team Member 5" class="team-member-image">
                    <div class="member-info">
                        <h3>Yue Huang</h3>
                        <!-- <p class="member-bio">Short bio describing this team member's background, expertise, and contributions to the ReadMe LLM project.</p> -->
                        <div class="social-links">
                            <a href="https://www.linkedin.com/in/yuehuang01/">
                                <img src="assets/images/linkedin-logo.png" alt="LinkedIn" class="social-icon">
                            </a>
                            <!-- <a href="#">GitHub</a> -->
                        </div>
                    </div>
                </div>

                <!-- Team Member 6 -->
                <div class="team-member">
                    <img src="assets/images/sahai.jpg" alt="Team Member 6" class="team-member-image">
                    <div class="member-info">
                        <h3>Anant Sahai</h3>
                        <div class="member-title">Advisor</div>
                        <!-- <p class="member-bio">Short bio describing this team member's background, expertise, and contributions to the ReadMe LLM project.</p> -->
                        <div class="social-links">
                            <a href="https://www.linkedin.com/in/anantsahai/">
                                <img src="assets/images/linkedin-logo.png" alt="LinkedIn" class="social-icon">
                            </a>
                            <!-- <a href="#">GitHub</a> -->
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!--         <section id="library">
            <h2>Library Makers</h2>
            <p><b>Challenge:</b> Loss of potential users to bigger alternative libraries AI is better at coding with.</p>
            <p><b>Action:</b> Create a ReadMe.LLM.</p>
            <p></p><b>Result:</b> More adoption and thus a fairer playing field.</p>
        </section> -->

        <!--         <section id="developer">
            <h2>Developers</h2>
            <p><b>Challenge:</b> Incorrect code causes frustration and wastes time and resources for debugging.</p>
            <p><b>Action:</b> Prompt LLM with ReadMe.LLM file.</p>
            <p></p><b>Result:</b> Seamless development and thus faster innovation.</p>
        </section> -->

        <section id="citation">
            <h2>Cite</h2>
            <div class="content-box visual" style="margin-top: 2rem; max-width: 100%">
                <div class="text-swatch" style="width:50rem; max-width: 100%;">
                    <p>
                        If you rely on ReadMe.LLM and artifacts, we request that you cite to the underlying paper.
                    </p>
                    <!-- <div class="citation">

                        <div class="bibtex-field">
                            @misc{zhang2024cybenchframeworkevaluatingcybersecurity,
                        </div>
                        <div class="bibtex-entry">
                            <div class="bibtex-field">
                                <span class="bibtex-label">title</span>
                                <span class="bibtex-equals">=</span>
                                <span class="bibtex-value">{Cybench: A Framework for Evaluating Cybersecurity
                                    Capabilities and Risks of Language Models},</span>
                            </div>
                            <div class="bibtex-field">
                                <span class="bibtex-label">author</span>
                                <span class="bibtex-equals">=</span>
                                <span class="bibtex-value">{Andy K. Zhang and Neil Perry and Riya Dulepet and Joey Ji
                                    and Celeste Menders and Justin W. Lin and Eliot Jones and Gashon Hussein and
                                    Samantha Liu and Donovan Jasper and Pura Peetathawatchai and Ari Glenn and Vikram
                                    Sivashankar and Daniel Zamoshchin and Leo Glikbarg and Derek Askaryar and Mike Yang
                                    and Teddy Zhang and Rishi Alluri and Nathan Tran and Rinnara Sangpisit and
                                    Polycarpos Yiorkadjis and Kenny Osele and Gautham Raghupathi and Dan Boneh and
                                    Daniel E. Ho and Percy Liang},</span>
                            </div>
                            <div class="bibtex-field">
                                <span class="bibtex-label">year</span>
                                <span class="bibtex-equals">=</span>
                                <span class="bibtex-value">{2024},</span>
                            </div>
                            <div class="bibtex-field">
                                <span class="bibtex-label">eprint</span>
                                <span class="bibtex-equals">=</span>
                                <span class="bibtex-value">{2408.08926},</span>
                            </div>
                            <div class="bibtex-field">
                                <span class="bibtex-label">archivePrefix</span>
                                <span class="bibtex-equals">=</span>
                                <span class="bibtex-value">{arXiv},</span>
                            </div>
                            <div class="bibtex-field">
                                <span class="bibtex-label">primaryClass</span>
                                <span class="bibtex-equals">=</span>
                                <span class="bibtex-value">{cs.CR},</span>
                            </div>
                            <div class="bibtex-field">
                                <span class="bibtex-label">url</span>
                                <span class="bibtex-equals">=</span>
                                <span class="bibtex-value">{https://arxiv.org/abs/2408.08926},</span>
                            </div>
                            <div class="bibtex-field">
                                <span class="bibtex-label">note</span>
                                <span class="bibtex-equals">=</span>
                                <span class="bibtex-value" id="bibtex-time">{Accessed: 2025-03-18},</span>
                            </div>
                            <div class="bibtex-field">
                                <span class="bibtex-end">}</span>
                            </div>
                        </div>

                    </div> -->
                </div>
            </div>
        </section>

        <section id="contact">
            <h2>Contact</h2>
            <p>readmellm.ucb@gmail.com</p>
        </section>

    </main>

    <footer>
        <p></p>
    </footer>

    <script src="assets/js/main.js"></script>
</body>

</html>